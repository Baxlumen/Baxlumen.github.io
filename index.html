<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="半醒半醉日复日，无风无雨年复年">
<meta property="og:type" content="website">
<meta property="og:title" content="山渐青">
<meta property="og:url" content="https://github.com/Baxlumen/Baxlumens.github.io.git/index.html">
<meta property="og:site_name" content="山渐青">
<meta property="og:description" content="半醒半醉日复日，无风无雨年复年">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="山渐青">
<meta name="twitter:description" content="半醒半醉日复日，无风无雨年复年">





  
  
  <link rel="canonical" href="https://github.com/Baxlumen/Baxlumens.github.io.git/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>山渐青</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="https://cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>
    <a href="https://github.com/Baxlumen" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">山渐青</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">这个人很懒，什么也没留下</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Navigationsleiste an/ausschalten">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-首页 menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-关于">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-标签">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-成就">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>成就</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-commonweal">

    
    
    
      
    

    

    <a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-angellist"></i> <br>Commonweal 404</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2021/03/08/paper-0308/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2021/03/08/paper-0308/" class="post-title-link" itemprop="url">C++面试问题笔记誊抄</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2021-03-08 21:58:20 / Geändert am: 22:15:43" itemprop="dateCreated datePublished" datetime="2021-03-08T21:58:20+08:00">2021-03-08</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1、-在main执行之前和之后执行的代码可能是什么？"><a href="#1、-在main执行之前和之后执行的代码可能是什么？" class="headerlink" title="1、 在main执行之前和之后执行的代码可能是什么？"></a>1、 在main执行之前和之后执行的代码可能是什么？</h1><p><strong>main函数执行之前</strong>主要就是初始化系统相关资源：</p>
<ul>
<li>设置栈指针</li>
<li>初始化静态 static 变量和 global 全局变量，即 .data 段的内容</li>
<li>将未初始化部分的全局变量赋初值：数值型 short, int ,long 等为 0，bool 为 false，指针为 NULL等等，即 .bss 字段的内容</li>
<li>全局对象的初始化，在 main 之前调用构造函数，这是可能会执行前的一些代码</li>
<li>将 main 函数的参数 argc argv 等传递给 main 函数，然后才真正开始运行 main 函数</li>
<li>attribute((constructor))</li>
</ul>
<p><strong>main 函数执行之后</strong></p>
<ul>
<li>全局对象的析构函数会在 main 函数之后执行</li>
<li>可以用 atexit 注册一个函数，它会在 main 之后执行</li>
<li>attribute((destructor))</li>
</ul>
<h1 id="2、-结构体内存对齐问题"><a href="#2、-结构体内存对齐问题" class="headerlink" title="2、 结构体内存对齐问题"></a>2、 结构体内存对齐问题</h1><ul>
<li>结构体成员按照声明顺序存储，第一个成员地址和整个结构体地址相同</li>
<li>未特殊说明时，按结构体中size最大的成员对齐 (若有double成员，按 8 字节对齐 (64位))</li>
</ul>
<h1 id="3、-指针和引用的区别"><a href="#3、-指针和引用的区别" class="headerlink" title="3、 指针和引用的区别"></a>3、 指针和引用的区别</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/12/27/Tensornetwork/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/12/27/Tensornetwork/" class="post-title-link" itemprop="url">Tensornetwork_1</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-12-27 13:23:50 / Geändert am: 13:26:09" itemprop="dateCreated datePublished" datetime="2019-12-27T13:23:50+08:00">2019-12-27</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在本节中，我们将介绍基本的线性代数运算以及如何使用张量网络创建它们。虽然一开始使用张量网络似乎比手工操作更复杂，但我们将使用本节中开发的技能开始构建和收缩非常复杂的张量网络，否则很难做到这一点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> tensornetwork <span class="keyword">as</span> tn</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Next, we add the nodes containing our vectors.</span></span><br><span class="line">a = tn.Node(np.ones(<span class="number">10</span>))</span><br><span class="line"><span class="comment"># Either tensorflow tensors or numpy arrays are fine.</span></span><br><span class="line">b = tn.Node(np.ones(<span class="number">10</span>))</span><br><span class="line"><span class="comment"># We "connect" these two nodes by their "0th" edges.</span></span><br><span class="line"><span class="comment"># This line is equal to doing `tn.connect(a[0], b[0])</span></span><br><span class="line"><span class="comment"># but doing it this way is much shorter.</span></span><br><span class="line">edge = a[<span class="number">0</span>] ^ b[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># Finally, we contract the edge, giving us our new node with a tensor</span></span><br><span class="line"><span class="comment"># equal to the inner product of the two earlier vectors</span></span><br><span class="line">c = tn.contract(edge)</span><br><span class="line"><span class="comment"># You can access the underlying tensor of the node via `node.tensor`.</span></span><br><span class="line"><span class="comment"># To convert a Eager mode tensorflow tensor into </span></span><br><span class="line">print(c.tensor)</span><br></pre></td></tr></table></figure>
<pre><code>10.0
</code></pre><h3 id="边缘中心连接-（Edge-centric-connection-）"><a href="#边缘中心连接-（Edge-centric-connection-）" class="headerlink" title="边缘中心连接 （Edge-centric connection.）"></a>边缘中心连接 （Edge-centric connection.）</h3><p>在TensorNetwork中创建节点时，该节点将自动填充悬挂的边。为了将两个节点连接在一起，我们实际上删除了节点中的两个悬挂边，并用标准/跟踪边替换它们。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = tn.Node(np.eye(<span class="number">2</span>))</span><br><span class="line"><span class="comment"># Notice that a[0] is actually an "Edge" type.</span></span><br><span class="line">print(<span class="string">"The type of a[0] is:"</span>, type(a[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># This is a dangling edge, so this method will </span></span><br><span class="line">print(<span class="string">"Is a[0] dangling?:"</span>, a[<span class="number">0</span>].is_dangling())</span><br></pre></td></tr></table></figure>
<pre><code>The type of a[0] is: &lt;class &apos;tensornetwork.network_components.Edge&apos;&gt;
Is a[0] dangling?: True
</code></pre><p>Now, let’s connect a[0] to a[1]. This will create a “trace” edge. 也就是一个二维矩阵，然后他的0轴和1轴</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">trace_edge = a[<span class="number">0</span>] ^ a[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># Notice now that a[0] and a[1] are actually the same edge.</span></span><br><span class="line">print(<span class="string">"Are a[0] and a[1] the same edge?:"</span>, a[<span class="number">0</span>] <span class="keyword">is</span> a[<span class="number">1</span>])</span><br><span class="line">print(<span class="string">"Is a[0] dangling?:"</span>, a[<span class="number">0</span>].is_dangling())</span><br></pre></td></tr></table></figure>
<pre><code>Are a[0] and a[1] the same edge?: True
Is a[0] dangling?: False
</code></pre><p>这里有个地方，a[0]在没有进行缩并时，它是悬挂状态的；但是进行了运算的，就不是悬挂状态了，这里注意！！！！！！！！</p>
<h3 id="Axis-naming-（轴命名）"><a href="#Axis-naming-（轴命名）" class="headerlink" title="Axis naming （轴命名）"></a>Axis naming （轴命名）</h3><p>有时，使用轴数是非常不容易的，并且很难跟踪某些边缘的目的。为了更简单，您可以选择为节点的每个轴添加一个名称。然后，您可以通过使用名称而不是数字进行索引来获取各自的边缘。  0轴是y方向，1轴x方向</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here, a[0] is a['alpha'] and a[1] is a['beta']</span></span><br><span class="line">a = tn.Node(np.eye(<span class="number">2</span>), axis_names=[<span class="string">'alpha'</span>, <span class="string">'beta'</span>])</span><br><span class="line">edge = a[<span class="string">'alpha'</span>] ^ a[<span class="string">'beta'</span>]</span><br><span class="line">result = tn.contract(edge)</span><br><span class="line">print(result.tensor)</span><br></pre></td></tr></table></figure>
<pre><code>2.0
</code></pre><h1 id="第二节-高级网络收缩"><a href="#第二节-高级网络收缩" class="headerlink" title="第二节  高级网络收缩"></a>第二节  高级网络收缩</h1><h3 id="避免跟踪边缘"><a href="#避免跟踪边缘" class="headerlink" title="避免跟踪边缘"></a>避免跟踪边缘</h3><p>虽然TensorNetwork库完全支持迹edge，但如果避免创建它们，则压缩时间总是更快。这是因为迹边只对底层矩阵的对角线求和，其余的值（占总值的大部分）只是垃圾。【【【【【这里具体看pad 的笔记！！！<br>你们两个都浪费了计算时间和内存，因为有这些无用的跟踪边。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_edge_at_a_time</span><span class="params">(a, b)</span>:</span></span><br><span class="line">  node1 = tn.Node(a)</span><br><span class="line">  node2 = tn.Node(b)</span><br><span class="line">  edge1 = node1[<span class="number">0</span>] ^ node2[<span class="number">0</span>]</span><br><span class="line">  edge2 = node1[<span class="number">1</span>] ^ node2[<span class="number">1</span>]</span><br><span class="line">  tn.contract(edge1)</span><br><span class="line">  result = tn.contract(edge2)</span><br><span class="line">  <span class="keyword">return</span> result.tensor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_contract_between</span><span class="params">(a, b)</span>:</span></span><br><span class="line">  node1 = tn.Node(a)</span><br><span class="line">  node2 = tn.Node(b)</span><br><span class="line">  node1[<span class="number">0</span>] ^ node2[<span class="number">0</span>]</span><br><span class="line">  node1[<span class="number">1</span>] ^ node2[<span class="number">1</span>]</span><br><span class="line">  <span class="comment"># This is the same as </span></span><br><span class="line">  <span class="comment"># tn.contract_between(node1, node2)</span></span><br><span class="line">  result = node1 @ node2</span><br><span class="line">  <span class="keyword">return</span> result.tensor</span><br><span class="line"></span><br><span class="line">a = np.ones((<span class="number">1000</span>, <span class="number">1000</span>))</span><br><span class="line">b = np.ones((<span class="number">1000</span>, <span class="number">1000</span>))</span><br><span class="line">print(<span class="string">"Running one_edge_at_a_time"</span>)</span><br><span class="line">%timeit one_edge_at_a_time(a, b)</span><br><span class="line">print(<span class="string">"Running use_cotract_between"</span>)</span><br><span class="line">%timeit use_contract_between(a, b)</span><br></pre></td></tr></table></figure>
<pre><code>Running one_edge_at_a_time
35.5 ms ± 1.45 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
Running use_cotract_between
10.2 ms ± 778 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><p>我们还有contract_parallel，它的作用与contract_between相同，只传递一条边而不是两个节点。这将压缩所有与给定边“平行”的边（意味着与给定边共享同一两个节点的所有边）。 </p>
<p>使用这两种方法都很好，他们也会做同样的事情。实际上，如果您查看源代码，contract_parallel实际上只是调用contract_between。:)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_contract_parallel</span><span class="params">(a, b)</span>:</span></span><br><span class="line">  node1 = tn.Node(a)</span><br><span class="line">  node2 = tn.Node(b)</span><br><span class="line">  edge = node1[<span class="number">0</span>] ^ node2[<span class="number">0</span>]</span><br><span class="line">  node1[<span class="number">1</span>] ^ node2[<span class="number">1</span>]</span><br><span class="line">  result = tn.contract_parallel(edge)</span><br><span class="line">  <span class="comment"># You can use `get_final_node` to make sure your network </span></span><br><span class="line">  <span class="comment"># is fully contracted.</span></span><br><span class="line">  <span class="keyword">return</span> result.tensor</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Running contract_parallel"</span>)</span><br><span class="line">%timeit use_contract_parallel(a, b)</span><br></pre></td></tr></table></figure>
<pre><code>Running contract_parallel
8.52 ms ± 287 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre><h3 id="复杂收缩"><a href="#复杂收缩" class="headerlink" title="复杂收缩"></a>复杂收缩</h3><p>还记得这种很难写的张量收缩吗？好吧，我们要用13行简单的代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here, we will contract the following shaped network.</span></span><br><span class="line"><span class="comment"># O - O</span></span><br><span class="line"><span class="comment"># | X |</span></span><br><span class="line"><span class="comment"># O - O</span></span><br><span class="line">a = tn.Node(np.ones((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">b = tn.Node(np.ones((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">c = tn.Node(np.ones((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">d = tn.Node(np.ones((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment"># Make the network fully connected.</span></span><br><span class="line">a[<span class="number">0</span>] ^ b[<span class="number">0</span>]</span><br><span class="line">a[<span class="number">1</span>] ^ c[<span class="number">1</span>]</span><br><span class="line">a[<span class="number">2</span>] ^ d[<span class="number">2</span>]</span><br><span class="line">b[<span class="number">1</span>] ^ d[<span class="number">1</span>]</span><br><span class="line">b[<span class="number">2</span>] ^ c[<span class="number">2</span>]</span><br><span class="line">c[<span class="number">0</span>] ^ d[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># We are using the "greedy" contraction algorithm.</span></span><br><span class="line"><span class="comment"># Other algorithms we support include "optimal" and "branch".</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Finding the optimial contraction order in the general case is NP-Hard,</span></span><br><span class="line"><span class="comment"># so there is no single algorithm that will work for every tensor network.</span></span><br><span class="line"><span class="comment"># However, there are certain kinds of networks that have nice properties that</span></span><br><span class="line"><span class="comment"># we can expliot to making finding a good contraction order easier.</span></span><br><span class="line"><span class="comment"># These types of contraction algorithms are in developement, and we welcome </span></span><br><span class="line"><span class="comment"># PRs!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># `tn.reachable` will do a BFS to get all of the nodes reachable from a given</span></span><br><span class="line"><span class="comment"># node or set of nodes.</span></span><br><span class="line"><span class="comment"># nodes = &#123;a, b, c, d&#125;</span></span><br><span class="line">nodes = tn.reachable(a)</span><br><span class="line">result = tn.contractors.greedy(nodes)</span><br><span class="line">print(result.tensor)</span><br></pre></td></tr></table></figure>
<pre><code>64.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To make connecting a network a little less verbose, we have included</span></span><br><span class="line"><span class="comment"># the NCon API aswell.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This example is the same as above.</span></span><br><span class="line">ones = np.ones((<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tn.ncon([ones, ones, ones, ones], </span><br><span class="line">        [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>], </span><br><span class="line">         [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>], </span><br><span class="line">         [<span class="number">2</span>, <span class="number">3</span>, <span class="number">6</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array(64.)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To specify dangling edges, simply use a negative number on that index.</span></span><br><span class="line"></span><br><span class="line">ones = np.ones((<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tn.ncon([ones, ones], [[<span class="number">-1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">-2</span>]])</span><br></pre></td></tr></table></figure>
<pre><code>array([[2., 2.],
       [2., 2.]])
</code></pre><h1 id="Section-3-Node-splitting-第3节：节点拆分"><a href="#Section-3-Node-splitting-第3节：节点拆分" class="headerlink" title="Section 3: Node splitting  第3节：节点拆分"></a>Section 3: Node splitting  第3节：节点拆分</h1><p>在这个colab的最后一部分，将介绍SVD节点分割方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To make the singular values very apparent, we will just take the SVD of a</span></span><br><span class="line"><span class="comment"># diagonal matrix.</span></span><br><span class="line">diagonal_array = np.array([[<span class="number">2.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">                           [<span class="number">0.0</span>, <span class="number">2.5</span>, <span class="number">0.0</span>],</span><br><span class="line">                           [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First, we will go over the simple split_node method.</span></span><br><span class="line">a = tn.Node(diagonal_array)</span><br><span class="line">u, vh, _ = tn.split_node(</span><br><span class="line">    a, left_edges=[a[<span class="number">0</span>]], right_edges=[a[<span class="number">1</span>]])</span><br><span class="line">print(<span class="string">"U node"</span>)</span><br><span class="line">print(u.tensor)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"V* node"</span>)</span><br><span class="line">print(vh.tensor)</span><br></pre></td></tr></table></figure>
<pre><code>U node
[[0.         1.41421356 0.        ]
 [1.58113883 0.         0.        ]
 [0.         0.         1.22474487]]

V* node
[[0.         1.58113883 0.        ]
 [1.41421356 0.         0.        ]
 [0.         0.         1.22474487]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Now, we can contract u and vh to get back our original tensor!</span></span><br><span class="line">print(<span class="string">"Contraction of U and V*:"</span>)</span><br><span class="line">print((u @ vh).tensor)</span><br></pre></td></tr></table></figure>
<pre><code>Contraction of U and V*:
[[2.  0.  0. ]
 [0.  2.5 0. ]
 [0.  0.  1.5]]
</code></pre><p>我们还可以用两种方法降低最低奇异值，<br>通过设置max_singular_values。这是我们希望保留的原始奇异值的最大数目。<br>通过设置max_trun_error。这是去除的奇异值之和的最大量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We can also drop the lowest singular values in 2 ways, </span></span><br><span class="line"><span class="comment"># 1. By setting max_singular_values. This is the maximum number of the original</span></span><br><span class="line"><span class="comment"># singular values that we want to keep.</span></span><br><span class="line">a = tn.Node(diagonal_array)</span><br><span class="line">u, vh, truncation_error = tn.split_node</span><br><span class="line">     a, left_edges=[a[<span class="number">0</span>]], right_edges=[a[<span class="number">1</span>]], max_singular_values=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Notice how the two largest singular values (2.0 and 2.5) remain</span></span><br><span class="line"><span class="comment"># but the smallest singular value (1.5) is removed.</span></span><br><span class="line">print((u @ vh).tensor)</span><br></pre></td></tr></table></figure>
<pre><code>[[2.  0.  0. ]
 [0.  2.5 0. ]
 [0.  0.  0. ]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">通过查看返回的截断误差，我们可以看到移除的奇异值的值</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># truncation_error is just a normal tensorflow tensor.</span></span><br><span class="line">print(truncation_error)</span><br></pre></td></tr></table></figure>
<pre><code>[1.5]
</code></pre><h1 id="Section-4-running-on-GPUs"><a href="#Section-4-running-on-GPUs" class="headerlink" title="Section 4: running on GPUs"></a>Section 4: running on GPUs</h1><p>为了在GPU上运行，我们建议使用JAX后端，因为它与numpy有几乎完全相同的API。<br>To get a GPU, go to Runtime -&gt; Change runtime type -&gt; GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_abc_trace</span><span class="params">(a, b, c)</span>:</span></span><br><span class="line">  an = tn.Node(a)</span><br><span class="line">  bn = tn.Node(b)</span><br><span class="line">  cn = tn.Node(c)</span><br><span class="line">  an[<span class="number">1</span>] ^ bn[<span class="number">0</span>]</span><br><span class="line">  bn[<span class="number">1</span>] ^ cn[<span class="number">0</span>]</span><br><span class="line">  cn[<span class="number">1</span>] ^ an[<span class="number">0</span>]</span><br><span class="line">  <span class="keyword">return</span> (an @ bn @ cn).tensor</span><br><span class="line"></span><br><span class="line">a = np.ones((<span class="number">4096</span>, <span class="number">4096</span>))</span><br><span class="line">b = np.ones((<span class="number">4096</span>, <span class="number">4096</span>))</span><br><span class="line">c = np.ones((<span class="number">4096</span>, <span class="number">4096</span>))</span><br><span class="line"></span><br><span class="line">tn.set_default_backend(<span class="string">"numpy"</span>)</span><br><span class="line">print(<span class="string">"Numpy Backend"</span>)</span><br><span class="line">%timeit calculate_abc_trace(a, b, c)</span><br><span class="line">tn.set_default_backend(<span class="string">"jax"</span>)</span><br><span class="line"><span class="comment"># Running with a GPU: 202 ms</span></span><br><span class="line"><span class="comment"># Running with a CPU: 2960 ms</span></span><br><span class="line">print(<span class="string">"JAX Backend"</span>)</span><br><span class="line">%timeit np.array(calculate_abc_trace(a, b, c))</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/11/26/norm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/11/26/norm/" class="post-title-link" itemprop="url">norm-范数</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-11-26 10:38:46 / Geändert am: 10:43:14" itemprop="dateCreated datePublished" datetime="2019-11-26T10:38:46+08:00">2019-11-26</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><a href="https://blog.csdn.net/zaishuiyifangxym/article/details/81673491" target="_blank" rel="noopener">https://blog.csdn.net/zaishuiyifangxym/article/details/81673491</a></p>
<p>关于向量范数与矩阵范数的详解。<img src="/images/image.png" alt></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/08/31/CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/31/CUDA/" class="post-title-link" itemprop="url">High Performance Computing with GPU</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-08-31 16:19:59" itemprop="dateCreated datePublished" datetime="2019-08-31T16:19:59+08:00">2019-08-31</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-09-08 14:43:10" itemprop="dateModified" datetime="2019-09-08T14:43:10+08:00">2019-09-08</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Part-I-Introduction-to-GPU"><a href="#Part-I-Introduction-to-GPU" class="headerlink" title="Part I Introduction to GPU"></a>Part I Introduction to GPU</h1><h2 id="1、GPU的发展"><a href="#1、GPU的发展" class="headerlink" title="1、GPU的发展"></a>1、GPU的发展</h2><h3 id="1-1-GPU与GPGPU"><a href="#1-1-GPU与GPGPU" class="headerlink" title="1.1 GPU与GPGPU"></a>1.1 GPU与GPGPU</h3><p>GPU，图形处理器，发展速度超过CPU，不仅具有高质量和高性能的图形处理能力，还可以用于通用计算。<br>用于通用计算的GPU，即GPGPU</p>
<h3 id="1-2、GPU的发展阶段"><a href="#1-2、GPU的发展阶段" class="headerlink" title="1.2、GPU的发展阶段"></a>1.2、GPU的发展阶段</h3><p>略</p>
<h2 id="2、GPU与CPU的比较"><a href="#2、GPU与CPU的比较" class="headerlink" title="2、GPU与CPU的比较"></a>2、GPU与CPU的比较</h2><h3 id="2-1、单核时代的摩尔定律"><a href="#2-1、单核时代的摩尔定律" class="headerlink" title="2.1、单核时代的摩尔定律"></a>2.1、单核时代的摩尔定律</h3><p>CPU的时钟频率每18个月翻一番；CPU的制造工艺逐渐接近物理极限；功耗和发热成为巨大的障碍</p>
<h3 id="2-2、GPU是多核技术的代表之一"><a href="#2-2、GPU是多核技术的代表之一" class="headerlink" title="2.2、GPU是多核技术的代表之一"></a>2.2、GPU是多核技术的代表之一</h3><p>在每一块芯片上集成多个较低功耗的核心；单个核芯频率基本不变(一般在1-3GHz)；设计中心转向到多核的集成技术；GPU是一种特殊的多核处理器</p>
<h3 id="2-3、GPGPU的优势"><a href="#2-3、GPGPU的优势" class="headerlink" title="2.3、GPGPU的优势"></a>2.3、GPGPU的优势</h3><p>CPU：更多的资源用于缓存和逻辑控制<br>GPU：更多的资源用于计算，适用于高并行性、大规模数据密集型、可预测的计算模式<br>(相比于CPU，GPU内有多个ALU 逻辑运算单元)</p>
<h3 id="2-4、GPU的设计目的"><a href="#2-4、GPU的设计目的" class="headerlink" title="2.4、GPU的设计目的"></a>2.4、GPU的设计目的</h3><p>GPU必须在有限的面积上实现超强的运算能力和极高的存储器带宽，因此需要大量执行单元来运行更多相对简单的线程，在当前线程等待数据时切换到另一个处于就绪状态等待计算的线程。</p>
<p>从本质上说，CPU属于向量机，GPU属于阵列机。</p>
<h3 id="2-5-CPU-GPU异构系统"><a href="#2-5-CPU-GPU异构系统" class="headerlink" title="2.5 CPU/GPU异构系统"></a>2.5 CPU/GPU异构系统</h3><p>GPU与CPU经北桥通过AGP或PCI-E总线连接，各自有独立的外部存储器，分别是主存(HostMemory)和显存(DeviceMemory)。其中，CPU负责逻辑性强的事物处理，GPU负责高密度的浮点计算。在使用GPU前，CPU必须通过北桥将数据传输到GPU的显存中；在GPU完成计算后，GPU再将结果数据返回给主机内存。两者之间的通信时必不可少的，由于接口PCI-E的通信限制，优化数据通信的开销是必不可少的。</p>
<h1 id="Part-II-GPU-Architecture"><a href="#Part-II-GPU-Architecture" class="headerlink" title="Part II GPU Architecture"></a>Part II GPU Architecture</h1><h2 id="1、已有的两类GPU结构"><a href="#1、已有的两类GPU结构" class="headerlink" title="1、已有的两类GPU结构"></a>1、已有的两类GPU结构</h2><h3 id="1-1、支持通用计算的两类GPU结构"><a href="#1-1、支持通用计算的两类GPU结构" class="headerlink" title="1.1、支持通用计算的两类GPU结构"></a>1.1、支持通用计算的两类GPU结构</h3><p>基于流处理器阵列的主流GPU结构，基于通用计算核心的GPU结构。前者相对于后者具有更高的聚合计算性能，后者在可编程性上具有巨大优势</p>
<h2 id="2、存储器层次结构"><a href="#2、存储器层次结构" class="headerlink" title="2、存储器层次结构"></a>2、存储器层次结构</h2><h2 id="3、线程组织结构"><a href="#3、线程组织结构" class="headerlink" title="3、线程组织结构"></a>3、线程组织结构</h2><h3 id="3-1-CUDA中的线程层次"><a href="#3-1-CUDA中的线程层次" class="headerlink" title="3.1 CUDA中的线程层次"></a>3.1 CUDA中的线程层次</h3><p>(CUDA是NVIDIA推出的运算平台，通用并行计算架构，该架构使GPU能够解决复杂的计算问题，它包含了CUDA指令集架构以及GPU内部的并行计算引擎)<br>线程：CUDA中的基本执行单元；硬件支持，开销很小；所有线程执行相同的代码(STMD)<br>线程块：若干线程组成线程块(block,每个块至多512个线程)；线程块可以呈一维、二维或者三维结构；每个线程块分为若干组(称为warp)，每个warp包含32个线程，物理上以SIMD方式并行。<br>线程网络：若干线程块可以组成网络(grid);Grid可以是一维或二维结构。</p>
<h3 id="3-2线程块ID和线程ID"><a href="#3-2线程块ID和线程ID" class="headerlink" title="3.2线程块ID和线程ID"></a>3.2线程块ID和线程ID</h3><p>线程ID：a)local id：线程ID在一个block里   b)global id:线程id在一个grid里<br>每一个线程使用ID来决定要处理的数据</p>
<h3 id="3-3线程块中的线程合作"><a href="#3-3线程块中的线程合作" class="headerlink" title="3.3线程块中的线程合作"></a>3.3线程块中的线程合作</h3><p>线程是通过共享内存在block中协作的，而在不同的block中是不能狗协作的</p>
<h2 id="4、同步"><a href="#4、同步" class="headerlink" title="4、同步"></a>4、同步</h2><h3 id="4-1、CPU与GPU之间的同步"><a href="#4-1、CPU与GPU之间的同步" class="headerlink" title="4.1、CPU与GPU之间的同步"></a>4.1、CPU与GPU之间的同步</h3><p>CPU的启动内核kernel是异步的，即当CPU启动GPU执行kernel时，CPU并不等待GPU完成就立即返回，继续执行后面的代码。例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kernel &lt;&lt;&lt;gridDim,blockDim&gt;&gt;&gt;(arg1,arg2);</span><br><span class="line">c=a+b</span><br></pre></td></tr></table></figure></p>
<p>CPU在调用kernel后，就接着执行后面的a+b，而GPU在执行kernel函数，此时CPU与GPU是完全的并行工作。如果CPU在接下来的操作中需要用到GPU的计算结果，则CPU必须阻塞等待GPU执行完毕，可在kernel后添加一条同步语句实现。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;gridDim,blockDim&gt;&gt;&gt;(arg1,arg2)</span><br><span class="line">cudaThreadSynchronize(); //实现CPU与GPU之间的同步</span><br><span class="line">c=a+b</span><br></pre></td></tr></table></figure></p>
<h3 id="4-2、同一个block内的同步"><a href="#4-2、同一个block内的同步" class="headerlink" title="4.2、同一个block内的同步"></a>4.2、同一个block内的同步</h3><p>前面提到，同一个block内的线程可以通过    shared memory共享数据。此外，同一个block内的线程还可以快速同步<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">__global__ void kernel(arg1,arg2)</span><br><span class="line">&#123;</span><br><span class="line">	int tid=threadldx.x;</span><br><span class="line">	...</span><br><span class="line">	__syncthreads();  //此函数用于实现同一个人块内的线程</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>只有当块内的所有线程都到达函数 syncthreads()时才会继续向下进行</p>
<h3 id="4-3、不同的block之间的同步"><a href="#4-3、不同的block之间的同步" class="headerlink" title="4.3、不同的block之间的同步"></a>4.3、不同的block之间的同步</h3><p>同一个grid中的不同block之间不能同步，即CUDA运行时库中没有提供此类函数，但是可以通过终止一个kernel来实现同步。</p>
<h1 id="PartIII-CUDA-Programming"><a href="#PartIII-CUDA-Programming" class="headerlink" title="PartIII CUDA Programming"></a>PartIII CUDA Programming</h1><h2 id="1-CUDA软件架构"><a href="#1-CUDA软件架构" class="headerlink" title="1.CUDA软件架构"></a>1.CUDA软件架构</h2><p>三个部分：(1)开发库(CUDA Library)，目前包括两个标准的数学运算库CUFFT和CUBLAS<br>         (2)运行时环境(CUDA Runtime)，提供开发接口和运行时的组件，包括基本数据类型的定义和各类计算、内存管理、设备访问和执行调度等函数。<br>         (3)驱动(CUDA Driver)，提供了GPU抽象级的访问接口，是的同一个CUDA应用可以    正确运行在所有支持CUDA的不同硬件上</p>
<h2 id="2-CUDA编程语言"><a href="#2-CUDA编程语言" class="headerlink" title="2.CUDA编程语言"></a>2.CUDA编程语言</h2><p>以C语言为主，增加了若干定义和指令</p>
<h3 id="2-1、函数限定符"><a href="#2-1、函数限定符" class="headerlink" title="2.1、函数限定符"></a>2.1、函数限定符</h3><p>函数类型限定符需要指定函数的执行位置(主机或设备)和函数调用者(通过主机或通过设备)；在设备上执行的函数受到一些限制，如函数参数的数目固定，无法声明静态变量，不支持递归调用等等；用<em>global</em>限定符定义的函数是从主机上调用设备函数的唯一方式，其调用是异步的，即立即返回。<br>函数限定符    在何处执行    从何处调用     特性<br><em>device</em>        设备          设备      函数地址无法获取<br><em>global</em>        设备          主机      返回类型必须为空<br><em>host</em>          主机          主机      等同于不使用任何限定符</p>
<h3 id="2-3、变量限定符"><a href="#2-3、变量限定符" class="headerlink" title="2.3、变量限定符"></a>2.3、变量限定符</h3><p>shared 限定符声明的变量只有在线程同步执行之后，才能保证共享变量对其他线程的正确性。<br>不带限定符的变量通常位于寄存器中。若寄存器不足，则置于本地存储器中。<br>限定符     位于何处        可以访问的线程            主机访问<br><em>device</em>   全局存储器  线程网格内的所有线程      通过运行时库访问<br><em>constant</em> 固定存储器  线程网格内的所有线程      通过运行时库访问<br><em>shared</em>   共享存储器  线程块内的所有线程        不可从主机访问</p>
<p>主机可以访问的变量：全局存储器，常量存储器<br>主机不可以访问的变量：寄存器、共享存储器、本地存储器</p>
<h3 id="2-3、内置的向量类型"><a href="#2-3、内置的向量类型" class="headerlink" title="2.3、内置的向量类型"></a>2.3、内置的向量类型</h3><p>内置的向量类型都是结构体，用(u)+基本数据结构类型+数字1-4组成，例如char2,uint3,ulong4等。<br>特殊类型dim3，基本等同于uint3，区别只在与定义变量dim3变量时，未指定的分量都自动初始化为1。一般用于定义线程块和线程网格的大小。</p>
<h3 id="2-4、常用的内置变量"><a href="#2-4、常用的内置变量" class="headerlink" title="2.4、常用的内置变量"></a>2.4、常用的内置变量</h3><p>内置变量   类型        含义<br>gridDim   dim3    线程网格的维度<br>blockDim  dim3    线程块的维度<br>blockldx  uint3   线程网格内块的索引<br>threadldx uint3   线程块内线程的索引<br>warpSize  int     一个warp块内包含的线程数</p>
<h2 id="3、内核函数"><a href="#3、内核函数" class="headerlink" title="3、内核函数"></a>3、内核函数</h2><h3 id="3-1、内核函数-Kernel"><a href="#3-1、内核函数-Kernel" class="headerlink" title="3.1、内核函数(Kernel)"></a>3.1、内核函数(Kernel)</h3><p>内核函数是特殊的一种函数，是从主机调用设备代码唯一的接口，相当于显卡环境中的主函数。<br>内核函数的参数被通过共享存储器传递，从而造成可用的共享存储器空间减少(一般减少100字节以内)。<br>内核函数使用<em>global</em>函数限定符声明，返回值为空<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">_global_ void KernelDemo(float*a.float*b,float*c)</span><br><span class="line">&#123;</span><br><span class="line">	int i =threadldx.x</span><br><span class="line">	c[i] = a[i] + b[i]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="3-2、内核函数-kernel-的调用"><a href="#3-2、内核函数-kernel-的调用" class="headerlink" title="3.2、内核函数(kernel)的调用"></a>3.2、内核函数(kernel)的调用</h3><p>调用内核函数需要使用KernelName&lt;&lt;&lt;&gt;&gt;&gt;()的方式。<br>&lt;&lt;&lt;&gt;&gt;&gt;  内的参数用于指定执行内核函数的配置，包括线程网络，线程块的维度，以及需求的共享内存大小，例如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;DimGrid,DimBlock,MenSize&gt;&gt;&gt;</span><br><span class="line">//DimGrid(dim3类型)，用于指定网络的两个维度，第三维被忽略</span><br><span class="line">//DimBlock(dim3类型)，指定线程块的三个维度</span><br><span class="line">//MemSize(size_t类型)，指定为此内核调用需要动态分配的共享存储器大小</span><br></pre></td></tr></table></figure></p>
<p>若当前硬件无法满足用户指定的配置，则内核函数不会被执行，直接返回错误信息。</p>
<h3 id="3-3、内核调用实例"><a href="#3-3、内核调用实例" class="headerlink" title="3.3、内核调用实例"></a>3.3、内核调用实例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">_global_ void KernelDemo(float*a,float*b,float*c) //内核定义</span><br><span class="line">&#123;</span><br><span class="line">	int i = threadldx.x;</span><br><span class="line">	c[i] = a[i] + b[i];</span><br><span class="line">&#125;</span><br><span class="line">int main()//主函数</span><br><span class="line">&#123;</span><br><span class="line">	dim3 dimGrid(1,1,1);</span><br><span class="line">	dim3 dimBlock(100,1,1);</span><br><span class="line">	KernelDemo&lt;&lt;&lt;dimGrid,dimBlock,1024&gt;&gt;&gt;(a,b,c); //调用内核</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="4、运行时API"><a href="#4、运行时API" class="headerlink" title="4、运行时API"></a>4、运行时API</h2><h3 id="4-1-运行时API"><a href="#4-1-运行时API" class="headerlink" title="4.1 运行时API"></a>4.1 运行时API</h3><p>设备管理：<br>·cudaGetDeviceCount():获得可用GPU设备的数目<br>·cudaGetDeviceProperties():得到相关的硬件属性<br>·使用cudaSetDevice():选择本次计算使用的设备<br>·默认使用第一个可用的GPU设备，即device 0</p>
<p>内存管理：<br>·cudaMalloc()：分配线性存储空间<br>·cudaFree():释放分配的空间<br>·cudaMemcpy():内存拷贝<br>·cudaMallocPitch():分配二维数组空间并自动对齐<br>·cudaMemcpyToSymbol():将主机上的一块数据复制到GPU上的固定存储器</p>
<h3 id="4-2、内存拷贝cudaMemcpy"><a href="#4-2、内存拷贝cudaMemcpy" class="headerlink" title="4.2、内存拷贝cudaMemcpy()"></a>4.2、内存拷贝cudaMemcpy()</h3><p>由于主机内存和设备内存是完全不同的两个内存空间，因此必须严格指定数据所在的位置。<br>四种不同的传输方式：<br>·主机到主机(Host To Host)<br>·主机到设备(Host To Device)<br>·设备到主机<br>·设备到设备<br>其中主机到设备和设备到主机的传输需要经过主板上的PCI-E总线接口，一般带宽在1-2GB/s左右。而设备到设备的带宽可达40GB/s以上</p>
<h3 id="4-3、计时函数"><a href="#4-3、计时函数" class="headerlink" title="4.3、计时函数"></a>4.3、计时函数</h3><p>CUDA自带一个精确的计时函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">unsigned int timer = 0;</span><br><span class="line">CUT_SAFE_CALL(cutCreateTimer(&amp;timer));//定义计时器</span><br><span class="line">cudaThreadSynchronize;// CPU与GPU的同步</span><br><span class="line">CUT_SAFE_CALL(cutStartTimer(timer));//计时器启动</span><br><span class="line"></span><br><span class="line">CudaKernel&lt;&lt;&lt;dimGrid,dimBlock,memsize&gt;&gt;&gt;();//GPU计算</span><br><span class="line"></span><br><span class="line">cudaThreadSynchronize(); //等待计算完成</span><br><span class="line">CUT_SAFE_CALL(cutStopTimer(timer));计时器停止</span><br><span class="line">float timecost=cutGetAverageTimerValue(timer);//获得计时结果</span><br><span class="line">printf(&quot;CUDA time %.3fms\n&quot;,timecost);</span><br></pre></td></tr></table></figure></p>
<h2 id="5、CUDA的程序结构"><a href="#5、CUDA的程序结构" class="headerlink" title="5、CUDA的程序结构"></a>5、CUDA的程序结构</h2><h3 id="5-1、-CUDA编程七步曲"><a href="#5-1、-CUDA编程七步曲" class="headerlink" title="5.1、 CUDA编程七步曲"></a>5.1、 CUDA编程七步曲</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cudaSetDevice(0);//获取设备；只有一个GPU时或默认使用0号GPU时可以省略</span><br><span class="line">cudaMalloc((void**) &amp;d_a,sizeof(float)*n);//分配显存</span><br><span class="line">cudaMemcpy(d_a,a,sizeof(float)*n,cudaMemcpyHostToDevice);//数据传输，H2D</span><br><span class="line">gpu_kernel&lt;&lt;&lt;blocks,threads&gt;&gt;&gt;(***);//kernel函数</span><br><span class="line">cudaMemcpy(a,d_a,sizeof(float)*n,cudaMemcpyDeviceToHost);//数据传输，D2H</span><br><span class="line">cudaFree(d_a); //释放显存空间</span><br><span class="line">cudaDeviceReset();//重置设备；可以省略</span><br></pre></td></tr></table></figure>
<p>(大概含义)一些连续的或者相对简单的部分在host上；复杂的部分在device上。</p>
<h3 id="5-1、CUDA程序生命周期"><a href="#5-1、CUDA程序生命周期" class="headerlink" title="5.1、CUDA程序生命周期"></a>5.1、CUDA程序生命周期</h3><p>(1)主机代码执行<br>(2)传输数据到GPU<br>(3)GPU执行<br>(4)传输数据回CPU<br>(5)继续主机代码执行<br>(6)结束<br>如果有多个内核函数，需要重复2-4步</p>
<h3 id="5-2、一个典型的CUDA程序"><a href="#5-2、一个典型的CUDA程序" class="headerlink" title="5.2、一个典型的CUDA程序"></a>5.2、一个典型的CUDA程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Main()&#123; //主函数</span><br><span class="line">float*Md;</span><br><span class="line">cudaMalloc((void**)&amp;Md,size);//在GPU上分配空间</span><br><span class="line">//从CPU复制数据到GPU</span><br><span class="line">cudaMemcpy(Md,M,size,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">//调用内核函数</span><br><span class="line">kernel&lt;&lt;&lt;dimGrid,dimBlock&gt;&gt;&gt;(arguments);</span><br><span class="line"></span><br><span class="line">//从GPU将结果复制回CPU</span><br><span class="line">CopyFromDeviceMatrix(M,Md);</span><br><span class="line">FreeDeviceMatrix(Md);//释放GPU上分配的空间</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="6、RuntimeAPI-设备管理部分"><a href="#6、RuntimeAPI-设备管理部分" class="headerlink" title="6、RuntimeAPI(设备管理部分)"></a>6、RuntimeAPI(设备管理部分)</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line">__global__ void kernelF()</span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	int count;</span><br><span class="line">	cudaGetDeviceCount(&amp;count);//返回计算能力大于等于1.0的GPU设备，注意里面的参数类型</span><br><span class="line">	kernelF&lt;&lt;&lt;1,10&gt;&gt;&gt;();</span><br><span class="line">        if(count==0)</span><br><span class="line">	&#123;</span><br><span class="line">		printf(&quot;没有计算能力大于等于1.0的GPU设备&quot;);</span><br><span class="line">	&#125;	</span><br><span class="line">	else</span><br><span class="line">	&#123;</span><br><span class="line">		printf(&quot;计算能力大于等于1.0的设备数：%d\n&quot;,count);</span><br><span class="line">	&#125;</span><br><span class="line">	int gpuid = 0;//选择0号GPU</span><br><span class="line">	cudaSetDevice(gpuid);//根据GPU索引号来决定使用那个GPU；如果没有指定，就默认使用0号</span><br><span class="line">	</span><br><span class="line">	int device;</span><br><span class="line">	cudaGetDevice(&amp;device);//获得当前线程所使用的GPU索引号,赋值给device，注意参数类型</span><br><span class="line">	printf(&quot;所使用的GPU索引号为：%d\n&quot;,device);</span><br><span class="line">	</span><br><span class="line">	cudaDeviceProp prop;//这里实际上prop是一个结构体变量，存储GPU的所有信息</span><br><span class="line">	cudaGetDeviceProperties(&amp;prop,device);//获取GPU索引号为device的参数信息到结构体数据prop中</span><br><span class="line">	printf(&quot;每块注册器的数量：%d\n&quot;,prop.regsPerBlock);</span><br><span class="line">	</span><br><span class="line">	int deviceIndex;</span><br><span class="line">        cudaChooseDevice(&amp;deviceIndex,&amp;prop);//根据prop的参数信息，选择设备最匹配的GPU，返回其索引号到deviceIndex</span><br><span class="line">        printf(&quot;最匹配的GPU索引号是：%d\n&quot;,deviceIndex);	</span><br><span class="line">	</span><br><span class="line">       // cudaSetValidDevices() 设置GPU设备列表        </span><br><span class="line"></span><br><span class="line">	return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/08/16/MachineLearning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/08/16/MachineLearning/" class="post-title-link" itemprop="url">MachineLearning笔记-1</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-08-16 16:31:23" itemprop="dateCreated datePublished" datetime="2019-08-16T16:31:23+08:00">2019-08-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Bearbeitet am</span>
                
                <time title="Geändert am: 2019-09-07 19:41:07" itemprop="dateModified" datetime="2019-09-07T19:41:07+08:00">2019-09-07</time>
              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一、引言-Introduction"><a href="#一、引言-Introduction" class="headerlink" title="一、引言(Introduction)"></a>一、引言(Introduction)</h1><h2 id="1、什么是MachineLearning"><a href="#1、什么是MachineLearning" class="headerlink" title="1、什么是MachineLearning"></a>1、什么是MachineLearning</h2><p> ·Arthur Samuel(1959). Macine Learning:Field of study that gives computers the ability to learn without being explicitly programmed</p>
<p> ·另一个人对于其定义是：一个程序被认为是从经验E中学习，解决任务T，达到性能度量值P，当且仅当有了经验E后，经过P评判，程序在处理T时的性能有所提升。</p>
<p>对于机器学习有许多算法，大致分为两类：监督学习(supervised learning)与非监督学习。监督学习是指我们主动去教计算机如何完成任务；非监督学习是我们打算让计算机自己去学习如何解决任务</p>
<h2 id="2、监督学习-Supervised-Learning"><a href="#2、监督学习-Supervised-Learning" class="headerlink" title="2、监督学习 Supervised Learning"></a>2、监督学习 Supervised Learning</h2><p>监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也成为监督训练或教师学习。大致可分为：回归问题，分类问题。<br>(根据已知的数据集去预测)</p>
<h2 id="3、无监督学习-Unsupervised-Learning"><a href="#3、无监督学习-Unsupervised-Learning" class="headerlink" title="3、无监督学习 Unsupervised Learning"></a>3、无监督学习 Unsupervised Learning</h2><p>如果说监督学习是已知一个数据集，对于其中的每个数据都知道它的”标签”(比如肿瘤是良性还是恶性)。但是对于无监督学习，已知一个数据集，但是没有任何标签或<br>者都是相同的标签(也就是无法分辨)。无监督学习能够判断出数据集有两个不同的聚集簇(聚类算法)</p>
<p>举例 鸡尾酒会问题：<br>一个环境中有两个人，两个麦克风，每个麦克风与两个人的距离不同。两人同时发出声音，麦克风会收录进两个人的声音。通过非监督学习的算法，将这两个人的声音给区别出来。<br>(也就是相当于给出一个数据集，通过非监督学习来分类)<br>可以用的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x&apos;); %svd 奇异值分解</span><br></pre></td></tr></table></figure></p>
<h1 id="二、单变量线性回归-Linear-regression-with-One-Variable"><a href="#二、单变量线性回归-Linear-regression-with-One-Variable" class="headerlink" title="二、单变量线性回归(Linear regression with One Variable)"></a>二、单变量线性回归(Linear regression with One Variable)</h1><h2 id="1、模型表示"><a href="#1、模型表示" class="headerlink" title="1、模型表示"></a>1、模型表示</h2><p>回归问题：回归一词是指，我们根据之前的数据预测出一个准确的输出值。同时，还有一种最常见的监督学习的方式，成为分类问题，例如预测肿瘤是良性还是恶性。。更进一步来说，在监督学习中我们有一个数据集，这个数据集被称为训练集。</p>
<p>符号规定<br>m=训练集中样本的数目<br>x=代表特征/输入变量<br>y=代表目标变量/输出变量<br>(x,y)=代表训练集中的实例<br>(x(i),y(i))=代表第i个观察实例 (这里是上角标)<br>h 代表学习算法的解决方案或函数，也成为假设(hypothesis)。<br>也就是h代表一个函数，假设，输入(x)是房屋尺寸的大小，输出(y)书对应房子的价格，因此，h就是y关于x的一个函数。但是这个函数并不是准确的反应两者之间的关系，所以成为hypothesis。</p>
<p>一种可能的表达 h=a+bx，也就是线性函数，因为只含有一个特征/输入变量，因此这样的问题也叫做单变量线性回归问题。</p>
<h2 id="2-代价函数"><a href="#2-代价函数" class="headerlink" title="2.代价函数"></a>2.代价函数</h2><p>(弄清楚最有可能的直线与数据相拟合)<br>在  h=a+bx中，参数a，b叫模型参数。a,b的不同，h(x)也就不同。那么就需要找到h(x)与y最相近的一组a,b。</p>
<p>最小化问题，也就是找到一组a,b，使得训练集中每一个x的函数值h(x)与实际值y  两者之差的平方，再相加，最后除以2m(m是训练集中样本数目)，使得这个最小。这里也就是所谓的代价函数。【这里贴图】。代价函数也被称为平方误差函数或平方误差代价函数。</p>
<p>当代价函数最小时，也就是选取的两个参数最好，即h=a+bx这条曲线能够拟合的最好。</p>
<h2 id="3、梯度下降"><a href="#3、梯度下降" class="headerlink" title="3、梯度下降"></a>3、梯度下降</h2><p>这里指的是batch梯度下降。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/06/24/numpy-4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/24/numpy-4/" class="post-title-link" itemprop="url">numpy相关_4</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-24 11:57:23 / Geändert am: 14:58:03" itemprop="dateCreated datePublished" datetime="2019-06-24T11:57:23+08:00">2019-06-24</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="一、-副本和视图"><a href="#一、-副本和视图" class="headerlink" title="一、 副本和视图"></a>一、 副本和视图</h1><p>副本是一个数据的完整拷贝，如果我们对副本进行修改，原数据不变，物理内存不再同一位置。<br>视图是数据的一个别称或引用，通过该别称或引用也可以访问、操作原有数据，但原有数据不会产生拷贝。如果我们对试图进行修改，会影响到原始数据，物理内存在同一位置。<br></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/06/24/numpy-4/#more" rel="contents">
                Weiterlesen &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/06/24/numpy-3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/24/numpy-3/" class="post-title-link" itemprop="url">numpy相关_3</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-24 11:57:19 / Geändert am: 20:02:04" itemprop="dateCreated datePublished" datetime="2019-06-24T11:57:19+08:00">2019-06-24</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="一、算数函数"><a href="#一、算数函数" class="headerlink" title="一、算数函数"></a>一、算数函数</h1><h2 id="1-加减乘除"><a href="#1-加减乘除" class="headerlink" title="1.加减乘除"></a>1.加减乘除</h2><p>add() subtract(),multiply(),divide()<br>需要注意的是数组必须具有相同的形状或符合数组广播的规则<br></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/06/24/numpy-3/#more" rel="contents">
                Weiterlesen &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/06/24/numpy-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/24/numpy-2/" class="post-title-link" itemprop="url">Numpy相关_2</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-24 11:57:12 / Geändert am: 13:22:11" itemprop="dateCreated datePublished" datetime="2019-06-24T11:57:12+08:00">2019-06-24</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="一、numpy广播-broadcast"><a href="#一、numpy广播-broadcast" class="headerlink" title="一、numpy广播(broadcast)"></a>一、numpy广播(broadcast)</h1><p>广播是numpy对于不同形状的数组进行数值计算的方式，对数组的算术运算通常在相应的元素上进行。但，如果两个数组ab的形状相同，那么a*b的结果就是a与b数组对应位相乘，这要求维数相同，且各维度的长度也相同<br></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/06/24/numpy-2/#more" rel="contents">
                Weiterlesen &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/06/24/numpy-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/24/numpy-1/" class="post-title-link" itemprop="url">Numpy相关_1</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-24 11:55:10 / Geändert am: 13:22:46" itemprop="dateCreated datePublished" datetime="2019-06-24T11:55:10+08:00">2019-06-24</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h1 id="一、numpy的ndarray对象"><a href="#一、numpy的ndarray对象" class="headerlink" title="一、numpy的ndarray对象"></a>一、numpy的ndarray对象</h1><p>numpy最重要的一个特点是其N维数组对象ndarray，他是乙烯类同类型数据的集合，以0下标为开始进行集合中元素的索引。<br></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/06/24/numpy-1/#more" rel="contents">
                Weiterlesen &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/Baxlumen/Baxlumens.github.io.git/2019/06/19/BaiduPCSGo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Baxlumen">
      <meta itemprop="description" content="半醒半醉日复日，无风无雨年复年">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="山渐青">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/06/19/BaiduPCSGo/" class="post-title-link" itemprop="url">BaiduPCSGo</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Veröffentlicht am</span>
              

              
                
              

              <time title="Erstellt: 2019-06-19 18:48:46 / Geändert am: 19:56:24" itemprop="dateCreated datePublished" datetime="2019-06-19T18:48:46+08:00">2019-06-19</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>摸鱼中像下载战国basara，但是网上找了一圈只有云盘的资源，3个G，无奈只好慢慢下。然后不知道是我的网的问题还是百度云的问题(貌似两个都有关系),速度奇慢。<br></p>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/2019/06/19/BaiduPCSGo/#more" rel="contents">
                Weiterlesen &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Nächste Seite"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Baxlumen">
            
              <p class="site-author-name" itemprop="name">Baxlumen</p>
              <div class="site-description motion-element" itemprop="description">半醒半醉日复日，无风无雨年复年</div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">25</span>
                    <span class="site-state-item-name">Artikel</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">14</span>
                    <span class="site-state-item-name">schlagwörter</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/Baxlumen" title="GitHub &rarr; https://github.com/Baxlumen"><i class="fa fa-fw fa-globe"></i>GitHub</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="298" height="52" src="//music.163.com/outchain/player?type=0&id=2021488376&auto=0&height=32"></iframe>
        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Baxlumen</span>

  

  
</div>


  <div class="powered-by">Erstellt mit  <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Design – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.1.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.1"></script>

  <script src="/js/motion.js?v=7.1.1"></script>



  
  


  <script src="/js/affix.js?v=7.1.1"></script>

  <script src="/js/schemes/pisces.js?v=7.1.1"></script>



  

  


  <script src="/js/next-boot.js?v=7.1.1"></script>


  

  

  

  



  




  

  

  

  

  

  

  

  

  

  

  

  

  

  

 <script src="/live2d-widget/autoload.js"></script>
</body>
</html>
